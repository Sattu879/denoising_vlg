{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM455hoLv4iqkwHT1HYpfVL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sattu879/denoising_vlg/blob/main/denoising.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eLMefbrhAqW",
        "outputId": "fc07fe5e-65d6-457e-abb5-91acbf317faa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Current directory: /content/drive/My Drive/Colab Notebooks\n",
            "Current directory: /content/drive/My Drive/Colab Notebooks/Train\n",
            "One or both of the directories do not exist. Please check the paths.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Step 1: Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Navigate to the current directory containing the notebook\n",
        "notebook_directory = '/content/drive/My Drive/Colab Notebooks'\n",
        "\n",
        "os.chdir(notebook_directory)\n",
        "\n",
        "# Verify the current directory\n",
        "print(\"Current directory:\", os.getcwd())\n",
        "\n",
        "# Step 3: Define the directory path\n",
        "test_directory = 'Train'\n",
        "os.chdir(test_directory)\n",
        "\n",
        "# Verify the current directory\n",
        "print(\"Current directory:\", os.getcwd())\n",
        "\n",
        "# Step 4: Define the paths to the high and low quality directories\n",
        "high_quality_dir = 'high'\n",
        "low_quality_dir = 'low'\n",
        "\n",
        "# Verify the directories exist\n",
        "if os.path.exists(high_quality_dir) and os.path.exists(low_quality_dir):\n",
        "    print(f\"'high' directory is located at: {os.path.abspath(high_quality_dir)}\")\n",
        "    print(f\"'low' directory is located at: {os.path.abspath(low_quality_dir)}\")\n",
        "else:\n",
        "    print(\"One or both of the directories do not exist. Please check the paths.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "high_quality_images = sorted(glob.glob(os.path.join(high_quality_dir, '*.png')))\n",
        "low_quality_images = sorted(glob.glob(os.path.join(low_quality_dir, '*.png')))\n",
        "\n",
        "assert len(high_quality_images) == len(low_quality_images), \"The number of images in both directories must be the same.\"\n",
        "\n",
        "\n",
        "print(f\"Found {len(high_quality_images)} high-quality images.\")\n",
        "print(f\"Found {len(low_quality_images)} low-quality images.\")\n",
        "\n",
        "# Function to load and pair images\n",
        "def load_image_pairs(high_quality_images, low_quality_images):\n",
        "    pairs = []\n",
        "    for hq_img_path, lq_img_path in zip(high_quality_images, low_quality_images):\n",
        "        hq_img = cv2.imread(hq_img_path, cv2.IMREAD_COLOR)\n",
        "        lq_img = cv2.imread(lq_img_path, cv2.IMREAD_COLOR)\n",
        "        if hq_img is None or lq_img is None:\n",
        "            print(f\"Error reading images: {hq_img_path}, {lq_img_path}\")\n",
        "        else:\n",
        "            pairs.append((hq_img, lq_img))\n",
        "    return pairs\n",
        "\n",
        "image_pairs = load_image_pairs(high_quality_images, low_quality_images)\n",
        "\n",
        "if len(image_pairs) == 0:\n",
        "    print(\"No image pairs were loaded. Check your file paths and image formats.\")\n",
        "else:\n",
        "    print(f\"Successfully loaded {len(image_pairs)} image pairs.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00L9J4LFhUtI",
        "outputId": "5ab16273-eb7c-461b-e502-4107f4690c1f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 high-quality images.\n",
            "Found 0 low-quality images.\n",
            "No image pairs were loaded. Check your file paths and image formats.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalization\n",
        "def normalize_image(image):\n",
        "    return image.astype(np.float32) / 255.0\n",
        "\n",
        "# Normalize all pairs\n",
        "normalized_pairs = [(normalize_image(hq), normalize_image(lq)) for hq, lq in image_pairs]\n",
        "\n",
        "print(f\"First normalized pair shapes: {normalized_pairs[0][0].shape}, {normalized_pairs[0][1].shape}\")\n",
        "\n",
        "high_quality_images, low_quality_images = zip(*normalized_pairs)\n",
        "\n",
        "hq_train, hq_test, lq_train, lq_test = train_test_split(\n",
        "    high_quality_images, low_quality_images, test_size=0.2, random_state=42)\n",
        "\n",
        "hq_train, hq_val, lq_train, lq_val = train_test_split(\n",
        "    hq_train, lq_train, test_size=0.25, random_state=42)\n",
        "\n",
        "\n",
        "hq_train, hq_val, hq_test = np.array(hq_train), np.array(hq_val), np.array(hq_test)\n",
        "lq_train, lq_val, lq_test = np.array(lq_train), np.array(lq_val), np.array(lq_test)\n",
        "\n",
        "print(\"Training set size:\", len(hq_train))\n",
        "print(\"Validation set size:\", len(hq_val))\n",
        "print(\"Test set size:\", len(hq_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "yYCOI-imhawj",
        "outputId": "b55ae094-a12f-46de-f706-0ee12b2c513f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-3006af4b16a9>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnormalized_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mhq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_pairs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"First normalized pair shapes: {normalized_pairs[0][0].shape}, {normalized_pairs[0][1].shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mhigh_quality_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_quality_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnormalized_pairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class ImagePairDataset(Dataset):\n",
        "    def __init__(self, hq_images, lq_images):\n",
        "        self.hq_images = hq_images\n",
        "        self.lq_images = lq_images\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.hq_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        hq_img = self.hq_images[idx]\n",
        "        lq_img = self.lq_images[idx]\n",
        "        hq_img = torch.from_numpy(hq_img.transpose((2, 0, 1)))\n",
        "        lq_img = torch.from_numpy(lq_img.transpose((2, 0, 1)))\n",
        "        return hq_img, lq_img\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 16\n",
        "train_dataset = ImagePairDataset(hq_train, lq_train)\n",
        "val_dataset = ImagePairDataset(hq_val, lq_val)\n",
        "test_dataset = ImagePairDataset(hq_test, lq_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "MMsw0Hclhqdo",
        "outputId": "ac514e3c-7d81-4202-f2e0-5533cd914863"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'hq_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-3a9db2941fdf>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Create data loaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImagePairDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhq_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlq_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImagePairDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhq_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlq_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImagePairDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhq_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlq_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'hq_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class DenoisingCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DenoisingCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 3, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.conv3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Ael9P5BNhtmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_model():\n",
        "    model = DenoisingCNN()\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    return model, criterion, optimizer"
      ],
      "metadata": {
        "id": "gVUeygTshxXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class ImagePairDataset(Dataset):\n",
        "    def __init__(self, hq_images, lq_images):\n",
        "        self.hq_images = hq_images\n",
        "        self.lq_images = lq_images\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.hq_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        hq_img = self.hq_images[idx]\n",
        "        lq_img = self.lq_images[idx]\n",
        "        hq_img = torch.from_numpy(hq_img.transpose((2, 0, 1))).float()\n",
        "        lq_img = torch.from_numpy(lq_img.transpose((2, 0, 1))).float()\n",
        "        return hq_img, lq_img\n",
        "\n",
        "def create_dataloaders(hq_train, lq_train, hq_val, lq_val, batch_size=16):\n",
        "    train_dataset = ImagePairDataset(hq_train, lq_train)\n",
        "    val_dataset = ImagePairDataset(hq_val, lq_val)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, val_loader"
      ],
      "metadata": {
        "id": "AYE8bpKBh5Eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=3, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
        "    model.to(device)\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for inputs, targets in train_loader:\n",
        "            print('It is working')\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs_val, targets_val in val_loader:\n",
        "                inputs_val, targets_val = inputs_val.to(device), targets_val.to(device)\n",
        "                outputs_val = model(inputs_val)\n",
        "                loss = criterion(outputs_val, targets_val)\n",
        "                val_loss += loss.item() * inputs_val.size(0)\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "    return model"
      ],
      "metadata": {
        "id": "1D_uC_DGh-E4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "batch_size = 16\n",
        "train_loader, val_loader = create_dataloaders(hq_train, lq_train, hq_val, lq_val, batch_size=batch_size)\n",
        "\n",
        "model, criterion, optimizer = initialize_model()\n",
        "\n",
        "# Train the model\n",
        "trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=2)\n",
        "\n",
        "# Save the model\n",
        "torch.save(trained_model.state_dict(), '/content/denoising_model.pth')\n"
      ],
      "metadata": {
        "id": "X8tIBQ13iBX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "model = DenoisingCNN()\n",
        "model.load_state_dict(torch.load('/content/denoising_model.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss = 0.0\n",
        "with torch.no_grad():\n",
        "    for inputs_test, targets_test in test_loader:\n",
        "        outputs_test = model(inputs_test.float())\n",
        "        loss = criterion(outputs_test, targets_test.float())\n",
        "        test_loss += loss.item() * inputs_test.size(0)\n",
        "\n",
        "test_loss /= len(test_loader.dataset)\n",
        "print(f'Test Loss: {test_loss:.4f}')\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "OdDD42u3iE-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
        "\n",
        "def calculate_metrics(denoised, original):\n",
        "    psnr_value = peak_signal_noise_ratio(original, denoised)\n",
        "    ssim_value, _ = structural_similarity(original, denoised, win_size=5, full=True, multichannel=True)\n",
        "    return psnr_value, ssim_value\n",
        "\n",
        "psnr_values = []\n",
        "ssim_values = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for idx, (lq_tensor, original_img) in enumerate(test_loader):\n",
        "        denoised_tensor = model(lq_tensor.float())\n",
        "        denoised_img = denoised_tensor.squeeze().cpu().numpy()\n",
        "        original_img = original_img.squeeze().cpu().numpy()\n"
      ],
      "metadata": {
        "id": "KGQQeijZiGE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import skimage.transform\n",
        "\n",
        "denoised_img = skimage.transform.resize(denoised_img, (500, 700))\n",
        "original_img = skimage.transform.resize(original_img, (500, 700))\n",
        ""
      ],
      "metadata": {
        "id": "kb3oCNBniMxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "psnr_value, ssim_value = calculate_metrics(denoised_img, original_img)"
      ],
      "metadata": {
        "id": "Wjj-9lmeiR_W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}